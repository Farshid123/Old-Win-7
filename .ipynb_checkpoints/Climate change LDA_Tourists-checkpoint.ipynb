{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This string has 183776 characters.\n"
     ]
    }
   ],
   "source": [
    "with open('Tourists_text2.txt','r') as myfile:\n",
    "    Tourists_text2=myfile.read().replace('\\n','')\n",
    "print(\"This string has\", len(Tourists_text2), \"characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Tourists_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_set = [Tourists_text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['review', 'tweet', 'one', 'of', 'the', 'great', 'drivesthis', 'is', 'often', 'rated', 'one', 'of', 'the', 'most', 'scenic', 'drives', 'in', 'the', 'world', 'and']\n"
     ]
    }
   ],
   "source": [
    "raw1 = Tourists_text2.lower()\n",
    "tokens1 = tokenizer.tokenize(raw1)\n",
    "print(tokens1[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python 3.6.5 :: Anaconda, Inc.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading https://files.pythonhosted.org/packages/52/d8/1a966940585bdd828d6ca8bca37d1be81e3e7e2fa1f51098117f15c32a1b/gensim-3.6.0-cp36-cp36m-win_amd64.whl (23.6MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\papar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\papar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\papar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from gensim) (1.14.3)\n",
      "Collecting smart-open>=1.2.1 (from gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n",
      "Requirement already satisfied: boto>=2.32 in c:\\users\\papar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim) (2.48.0)\n",
      "Collecting bz2file (from smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
      "Requirement already satisfied: requests in c:\\users\\papar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
      "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/c8/9f4489d72515dfdcd798a71fbebb885820ec60491f92da25045d3b3b5d12/boto3-1.9.7-py2.py3-none-any.whl (128kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\papar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\papar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\papar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\papar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (2018.4.16)\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
      "Collecting botocore<1.13.0,>=1.12.7 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/a6/93/25b23c1b853d2a6d5e597eb7ff5b9760f8f7fe98fb4469a6cf1fa9da597d/botocore-1.12.7-py2.py3-none-any.whl (4.7MB)\n",
      "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\users\\papar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.7->boto3->smart-open>=1.2.1->gensim) (2.7.3)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\users\\papar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.7->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open: started\n",
      "  Running setup.py bdist_wheel for smart-open: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Papar\\AppData\\Local\\pip\\Cache\\wheels\\23\\00\\44\\e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n",
      "  Running setup.py bdist_wheel for bz2file: started\n",
      "  Running setup.py bdist_wheel for bz2file: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Papar\\AppData\\Local\\pip\\Cache\\wheels\\81\\75\\d6\\e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: bz2file, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.9.7 botocore-1.12.7 bz2file-0.98 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lda\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/fe/98cec39e45411a83430978ea2ad21a47468875013e3c3512a9aef9afc46f/lda-1.1.0-cp36-cp36m-win_amd64.whl (334kB)\n",
      "Collecting pbr<4,>=0.6 (from lda)\n",
      "  Downloading https://files.pythonhosted.org/packages/0c/5d/b077dbf309993d52c1d71e6bf6fe443a8029ea215135ebbe0b1b10e7aefc/pbr-3.1.1-py2.py3-none-any.whl (99kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.13.0 in c:\\users\\papar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from lda) (1.14.3)\n",
      "Installing collected packages: pbr, lda\n",
      "Successfully installed lda-1.1.0 pbr-3.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\papar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (3.3)\n",
      "Requirement already satisfied: six in c:\\users\\papar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from nltk) (1.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stop_words\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
      "Building wheels for collected packages: stop-words\n",
      "  Running setup.py bdist_wheel for stop-words: started\n",
      "  Running setup.py bdist_wheel for stop-words: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Papar\\AppData\\Local\\pip\\Cache\\wheels\\75\\37\\6a\\2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
      "Successfully built stop-words\n",
      "Installing collected packages: stop-words\n",
      "Successfully installed stop-words-2018.7.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Papar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "import conda\n",
    "import nltk\n",
    "import lda\n",
    "import lda.datasets\n",
    "import gensim\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stop_words\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['review', 'tweet', 'one', 'great', 'drivesthis', 'often', 'rated', 'one', 'scenic', 'drives', 'world', 'certainly', 'one', 'scenic', 've', 'ever', 'taken', 'gorgeous', 'mountains', 'around']\n"
     ]
    }
   ],
   "source": [
    "# remove stop words from tokens\n",
    "stopped_tokens1 = [i for i in tokens1 if not i in en_stop]\n",
    "\n",
    "print(stopped_tokens1[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['review', 'tweet', 'one', 'great', 'drivesthi', 'often', 'rate', 'one', 'scenic', 'drive', 'world', 'certainli', 'one', 'scenic', 've', 'ever', 'taken', 'gorgeou', 'mountain', 'around']\n"
     ]
    }
   ],
   "source": [
    "# stem token\n",
    "stemmed_stopped_token1= [p_stemmer.stem(i) for i in stopped_tokens1]\n",
    "\n",
    "print(stemmed_stopped_token1[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stemmed_stopped_token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "print(en_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Topic Modeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Applying these steps on the whole text (stemmed_stopped_token1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.036*\"icefield\" + 0.032*\"trail\" + 0.032*\"inform\" + 0.028*\"place\" + 0.028*\"around\" + 0.025*\"back\" + 0.023*\"nation\" + 0.023*\"athabasca\" + 0.020*\"centr\" + 0.017*\"quit\" + 0.014*\"pretti\" + 0.014*\"short\" + 0.014*\"driver\" + 0.013*\"crowd\" + 0.013*\"vehicl\"'), (1, '0.069*\"warm\" + 0.049*\"wildfir\" + 0.029*\"make\" + 0.026*\"worth\" + 0.026*\"melt\" + 0.025*\"banff\" + 0.024*\"trip\" + 0.018*\"onto\" + 0.018*\"canyon\" + 0.017*\"mani\" + 0.013*\"smoke\" + 0.012*\"pass\" + 0.012*\"wonder\" + 0.012*\"attract\" + 0.012*\"thing\"'), (2, '0.072*\"year\" + 0.064*\"global\" + 0.063*\"get\" + 0.047*\"will\" + 0.028*\"amaz\" + 0.019*\"good\" + 0.017*\"now\" + 0.014*\"know\" + 0.014*\"ticket\" + 0.014*\"sign\" + 0.014*\"start\" + 0.013*\"opportun\" + 0.013*\"give\" + 0.013*\"far\" + 0.012*\"stun\"'), (3, '0.111*\"ice\" + 0.093*\"park\" + 0.081*\"walk\" + 0.051*\"time\" + 0.032*\"peopl\" + 0.031*\"day\" + 0.025*\"even\" + 0.024*\"drive\" + 0.022*\"still\" + 0.020*\"use\" + 0.018*\"sinc\" + 0.016*\"cold\" + 0.016*\"visitor\" + 0.015*\"didn\" + 0.014*\"ride\"'), (4, '0.235*\"glacier\" + 0.075*\"see\" + 0.044*\"climat\" + 0.037*\"tour\" + 0.034*\"go\" + 0.034*\"way\" + 0.026*\"us\" + 0.023*\"road\" + 0.016*\"columbia\" + 0.014*\"explor\" + 0.013*\"close\" + 0.011*\"seen\" + 0.010*\"snow\" + 0.009*\"gone\" + 0.009*\"say\"'), (5, '0.093*\"s\" + 0.027*\"stop\" + 0.025*\"retreat\" + 0.021*\"parkway\" + 0.020*\"ago\" + 0.019*\"last\" + 0.018*\"jasper\" + 0.015*\"took\" + 0.014*\"center\" + 0.014*\"need\" + 0.012*\"river\" + 0.011*\"fire\" + 0.010*\"skywalk\" + 0.010*\"later\" + 0.009*\"small\"'), (6, '0.057*\"visit\" + 0.056*\"bu\" + 0.050*\"one\" + 0.046*\"view\" + 0.043*\"beauti\" + 0.033*\"due\" + 0.023*\"canada\" + 0.021*\"fall\" + 0.021*\"easi\" + 0.020*\"part\" + 0.020*\"want\" + 0.017*\"enjoy\" + 0.017*\"along\" + 0.016*\"actual\" + 0.016*\"big\"'), (7, '0.053*\"hike\" + 0.043*\"area\" + 0.035*\"rece\" + 0.026*\"guid\" + 0.023*\"natur\" + 0.020*\"bit\" + 0.020*\"don\" + 0.019*\"sure\" + 0.017*\"sad\" + 0.014*\"huge\" + 0.013*\"recommend\" + 0.013*\"show\" + 0.012*\"effect\" + 0.012*\"highway\" + 0.012*\"larg\"'), (8, '0.072*\"lake\" + 0.061*\"chang\" + 0.041*\"much\" + 0.031*\"great\" + 0.027*\"water\" + 0.027*\"well\" + 0.026*\"also\" + 0.023*\"mountain\" + 0.023*\"like\" + 0.019*\"flood\" + 0.018*\"rocki\" + 0.017*\"must\" + 0.016*\"littl\" + 0.016*\"2\" + 0.016*\"first\"'), (9, '0.069*\"t\" + 0.068*\"field\" + 0.066*\"can\" + 0.055*\"take\" + 0.046*\"lot\" + 0.044*\"just\" + 0.036*\"experi\" + 0.026*\"realli\" + 0.018*\"tourist\" + 0.018*\"think\" + 0.016*\"love\" + 0.015*\"interest\" + 0.012*\"buse\" + 0.011*\"access\" + 0.011*\"edg\"')]\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in stemmed_stopped_token1:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=20)\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
